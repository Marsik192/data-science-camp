{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=  (379, 1)\n",
      "y_train.shape=  (379,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marsi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "df = pd.DataFrame (X, columns= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "df = df[['RM']]\n",
    "df['target'] = y\n",
    "\n",
    "# YOUR_CODE. select the values of feature 5 only (corresponding to 'RM') and assign to X \n",
    "# START_CODE \n",
    "X= np.array(df[['RM']])\n",
    "# END_CODE \n",
    "\n",
    "\n",
    "X= X.reshape(-1,1) # make it 2d as for case of mutivariable\n",
    "\n",
    "# YOUR_CODE. Apply train_test_split to X and Y to get X_train, X_test, y_train, y_test\n",
    "# START_CODE \n",
    "X_train, X_test, y_train, y_test=  train_test_split(X, y, random_state=2022)\n",
    "# END_CODE \n",
    "\n",
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('y_train.shape= ',y_train.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b= -0.276767596147759, \n",
      "w= [[0.581851]], \n",
      "X= \n",
      "[[ 2.14839926]\n",
      " [-1.279487  ]\n",
      " [ 0.50227689]\n",
      " [ 0.8560293 ]\n",
      " [-0.14279008]\n",
      " [ 0.11007867]\n",
      " [-0.68806479]\n",
      " [ 0.43356408]\n",
      " [ 0.510221  ]\n",
      " [-0.16513097]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97328067, -1.02123839,  0.01548272,  0.22131391, -0.35985014,\n",
       "        -0.21271821, -0.67711878, -0.0244979 ,  0.02010501, -0.37284922]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear_Regression_1():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def h(self, b, X, w):\n",
    "        assert (X.shape[1] == w.shape[1])\n",
    "        h_res =  b + X@w.T \n",
    "        return h_res\n",
    "\n",
    "\n",
    "\n",
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2018)\n",
    "b_check= np.random.randn()\n",
    "w_check= np.random.randn(1,1)\n",
    "X_check= np.random.randn(10,1)\n",
    "print('b= {}, \\nw= {}, \\nX= \\n{}'.format(b_check, w_check, X_check))\n",
    "lin_reg_1 = Linear_Regression_1()\n",
    "lin_reg_1.h(b_check, w_check, X_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y= [[-0.21767896]\n",
      " [ 0.82145535]\n",
      " [ 1.48127781]\n",
      " [ 1.33186404]\n",
      " [-0.36186537]\n",
      " [ 0.68560883]\n",
      " [ 0.57376143]\n",
      " [ 0.28772767]\n",
      " [-0.23563426]\n",
      " [ 0.95349024]], \n",
      "h= [[-1.6896253 ]\n",
      " [-0.34494271]\n",
      " [ 0.0169049 ]\n",
      " [-0.51498352]\n",
      " [ 0.24450929]\n",
      " [-0.18931261]\n",
      " [ 2.67217242]\n",
      " [ 0.46480249]\n",
      " [ 0.84593044]\n",
      " [-0.50354158]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.897146515186598"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear_Regression_2():\n",
    "    '''linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')    \n",
    "   \n",
    "        # YOUR_CODE. Assign expression for J to J_res \n",
    "        # START_CODE \n",
    "        J_res = 1/(2*m)*np.sum((h-y)**2)\n",
    "        # END_CODE       \n",
    "        return J_res \n",
    "\n",
    "np.random.seed(2019)\n",
    "m = 10 \n",
    "y_check= np.random.randn(m,1)\n",
    "h_check= np.random.randn(m,1)\n",
    "print('y= {}, \\nh= {}'.format(y_check, h_check))\n",
    "lin_reg_2 = Linear_Regression_2()\n",
    "lin_reg_2.m = m \n",
    "lin_reg_2.J(h_check, y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[-1.76884571]\n",
      " [ 0.07555227]\n",
      " [-1.1306297 ]\n",
      " [-0.65143017]\n",
      " [-0.89311563]\n",
      " [-1.27410098]\n",
      " [-0.06115443]\n",
      " [ 0.06451384]\n",
      " [ 0.41011295]\n",
      " [-0.57288249]], \n",
      "y= [[-0.80133362]\n",
      " [ 1.31203519]\n",
      " [ 1.27469887]\n",
      " [-1.2143576 ]\n",
      " [ 0.31371941]\n",
      " [-1.44482142]\n",
      " [-0.3689613 ]\n",
      " [-0.76922658]\n",
      " [ 0.3926161 ]\n",
      " [ 0.05729383]], \n",
      "b= 2.0899788404287745 \n",
      "w= [[0.04197131]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.1904608819958713, array([[-1.43284262]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear_Regression_3():\n",
    "    def __init__(self, max_iter = 1e5, alpha = 1,eps = 1e-10, verbose= 0):\n",
    "        pass        \n",
    "\n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Insert the expression of h developed in Linear_Regression_1\n",
    "        # START_CODE \n",
    "        h_res =  b + X@w.T \n",
    "        # END_CODE\n",
    "\n",
    "        return h_res\n",
    "        \n",
    "    def J_derivative(self, params , X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''    \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "\n",
    "        # YOUR_CODE. Assign expressions for derivates of J by b and by w  to dJ_b and dJ_w corrrespondingly       \n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m * np.sum(h_val-y)\n",
    "        dJ_w= 1/m * (h_val-y).T @ X\n",
    "        # END_CODE\n",
    "        \n",
    "        return (dJ_b, dJ_w)\n",
    "\n",
    "np.random.seed(2020)\n",
    "m = 10\n",
    "n = 1\n",
    "X_check= np.random.randn(m,n)\n",
    "y_check= np.random.randn(m,1)\n",
    "b_check= np.random.randn()\n",
    "w_check= np.random.randn(1,n)\n",
    "params = b_check, w_check\n",
    "print('X= {}, \\ny= {}, \\nb= {} \\nw= {}'.format(X_check, y_check, b_check, w_check))\n",
    "\n",
    "lin_reg_3 = Linear_Regression_3()\n",
    "lin_reg_3.m = m \n",
    "lin_reg_3.n = n \n",
    "lin_reg_3.J_derivative(params , X_check, y_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [[ 1.48860905]\n",
      " [ 0.67601087]\n",
      " [-0.41845137]\n",
      " [-0.80652081]\n",
      " [ 0.55587583]\n",
      " [-0.70550429]\n",
      " [ 1.13085826]\n",
      " [ 0.64500184]\n",
      " [ 0.10641374]\n",
      " [ 0.42215483]], \n",
      "y= [[ 0.12420684]\n",
      " [-0.83795346]\n",
      " [ 0.4090157 ]\n",
      " [ 0.10275122]\n",
      " [-1.90772239]\n",
      " [ 1.1002243 ]\n",
      " [-1.40232506]\n",
      " [-0.22508127]\n",
      " [-1.33620597]\n",
      " [ 0.30372151]]\n",
      "Running gradient descent with alpha = 1, eps= 1e-10, max_iter= 5\n",
      "b = -0.36693685587288444, w= [[-0.4217246]], J= 0.33976525493056825\n",
      "b = -0.23643637277401236, w= [[-0.46886908]], J= 0.3278115023016167\n",
      "b = -0.22184776004990137, w= [[-0.52721539]], J= 0.3250909705515032\n",
      "b = -0.20379279582278398, w= [[-0.55396166]], J= 0.32428457786538833\n",
      "b = -0.19551630227029396, w= [[-0.5697399]], J= 0.32403801171263197\n",
      "b = -0.19063380881762437, w= [[-0.57831305]], J= 0.3239623872203208\n",
      "b = -0.18798089094052142, w= [[-0.58309057]], J= 0.3239391853771439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear_Regression_4():\n",
    "    '''\n",
    "    linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self, max_iter = 1e5, alpha = 0.01,eps = 1e-10, verbose= 0):\n",
    "        '''\n",
    "        :param verbose: set 1 to display more details of J val changes\n",
    "        '''\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose       \n",
    "        \n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1] == w.shape[1])\n",
    "\n",
    "        h_res =  b + X @ w.T \n",
    "        \n",
    "        if h_res.shape != (X.shape[0],1):\n",
    "            print('h.shape = {} but expected {}'.format (h_res.shape,  (self.m,1)))\n",
    "            raise Exception('Check assertion in h')    \n",
    "        return h_res\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')   \n",
    "        # YOUR_CODE. Insert the expression of J developed in Linear_Regression_2\n",
    "        # START_CODE \n",
    "        J_res= 1/(2*m)*np.sum((h-y)**2)\n",
    "        # END_CODE \n",
    "\n",
    "        return J_res\n",
    "        \n",
    "    def J_derivative(self, params, X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''\n",
    "      \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "        \n",
    "        # YOUR_CODE. Insert the expressions for derivates of J by b and by w to dJ_b and dJ_w developed in Linear_Regression_3\n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m * np.sum(h_val-y)\n",
    "        dJ_w= 1/m * (h_val-y).T @ X\n",
    "        # END_CODE\n",
    "        \n",
    "        return (dJ_b, dJ_w)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X - ndarray training set of shape [m,n], m - number of samples, n - number of features\n",
    "        :param y - ndarray - 1d array \n",
    "        :return: True in case of successful fit \n",
    "        '''      \n",
    "        if self.verbose: \n",
    "            print ('Running gradient descent with alpha = {}, eps= {}, max_iter= {}'.format(\n",
    "                self.alpha, self.eps, self.max_iter))\n",
    "        self.m,self.n= X.shape # number of samples, number of features  \n",
    "        y = y.reshape(self.m,1) # make it 2 d to make sure it corresponds to h_val\n",
    "        b = 0 # init intercept with 0\n",
    "        w= np.zeros(self.n).reshape(1,-1) # make sure it's shape is [1,n]\n",
    "        params = (b,w)\n",
    "        \n",
    "        self.J_hist=[-1] # used for keeping J values. Init with -1 to avoid 0 at first iter\n",
    "        continue_iter = True # flag to continue next iter (grad desc step)\n",
    "        iter_number =0 # used for limit by max_iter\n",
    "\n",
    "        while continue_iter:            \n",
    "            # Do step of gradient descent    \n",
    "            # YOUR_CODE. Develop one step of gradien descent \n",
    "            # START_CODE \n",
    "            dJ_b, dJ_w = 1/m * np.sum(self.h(b,w,X)-y), 1/m * (self.h(b,w,X)-y).T @ X\n",
    "            b = b - self.alpha * dJ_b\n",
    "            w = w - self.alpha * dJ_w\n",
    "            params= b,w\n",
    "            # END_CODE \n",
    "            \n",
    "            # keep history of J values\n",
    "            self.J_hist.append(self.J(self.h(b, w, X), y))\n",
    "            if self.verbose:\n",
    "                print ('b = {}, w= {}, J= {}'.format(b,w,self.J_hist[-1]))\n",
    "            # check criteria of exit the loop (finish grad desc)\n",
    "            if self.max_iter and iter_number> self.max_iter: # if max_iter is provided and limit succeeded\n",
    "                continue_iter = False\n",
    "            elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
    "                continue_iter = False\n",
    "            iter_number += 1\n",
    "            \n",
    "        # store the final params to further using \n",
    "        self.intercept_, self.coef_= params        \n",
    "        return True\n",
    "\n",
    "\n",
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "np.random.seed(2021)\n",
    "m = 10 \n",
    "n = 1\n",
    "X_check= np.random.randn(m,n)\n",
    "y_check= np.random.randn(m,1)\n",
    "print('X= {}, \\ny= {}'.format(X_check, y_check))\n",
    "lin_reg_4 = Linear_Regression_4(alpha = 1, max_iter = 5, verbose=1)\n",
    "lin_reg_4.fit(X_check, y_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=  (379, 1)\n",
      "y_train.shape=  (379,)\n",
      "X_train= \n",
      "[[6.376]\n",
      " [6.749]\n",
      " [6.301]\n",
      " [5.88 ]\n",
      " [6.174]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marsi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\marsi\\AppData\\Local\\Temp\\ipykernel_33004\\3300570684.py:44: RuntimeWarning: overflow encountered in square\n",
      "  J_res= 1/(2*m)*np.sum((h-y)**2)\n",
      "C:\\Users\\marsi\\AppData\\Local\\Temp\\ipykernel_33004\\3300570684.py:108: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
      "C:\\Users\\marsi\\AppData\\Local\\Temp\\ipykernel_33004\\3300570684.py:95: RuntimeWarning: overflow encountered in matmul\n",
      "  dJ_b, dJ_w = 1/m * np.sum(self.h(b,w,X)-y), 1/m * (self.h(b,w,X)-y).T @ X\n",
      "C:\\Users\\marsi\\AppData\\Local\\Temp\\ipykernel_33004\\3300570684.py:96: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  b = b - self.alpha * dJ_b\n",
      "C:\\Users\\marsi\\AppData\\Local\\Temp\\ipykernel_33004\\3300570684.py:97: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - self.alpha * dJ_w\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 154\u001b[0m\n\u001b[0;32m    152\u001b[0m lin_reg\u001b[39m.\u001b[39mfit (X_train, y_train)\n\u001b[0;32m    153\u001b[0m lin_reg\u001b[39m.\u001b[39mdraw_cost_changes()\n\u001b[1;32m--> 154\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mR2 Score =\u001b[39m\u001b[39m'\u001b[39m, lin_reg\u001b[39m.\u001b[39;49mscore(X_test, y_test))\n\u001b[0;32m    155\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mb: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, w= \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(lin_reg\u001b[39m.\u001b[39mintercept_, lin_reg\u001b[39m.\u001b[39mcoef_)) \n",
      "Cell \u001b[1;32mIn [6], line 143\u001b[0m, in \u001b[0;36mLinear_Regression.score\u001b[1;34m(self, X_test, y_test)\u001b[0m\n\u001b[0;32m    141\u001b[0m z\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m r2_score\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m (r2_score(y_test, z))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_regression.py:911\u001b[0m, in \u001b[0;36mr2_score\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mr2_score\u001b[39m(\n\u001b[0;32m    785\u001b[0m     y_true,\n\u001b[0;32m    786\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m     force_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    791\u001b[0m ):\n\u001b[0;32m    792\u001b[0m     \u001b[39m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \n\u001b[0;32m    794\u001b[0m \u001b[39m    Best possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[39m    -inf\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    912\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    913\u001b[0m     )\n\u001b[0;32m    914\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    916\u001b[0m     \u001b[39mif\u001b[39;00m _num_samples(y_pred) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    101\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m--> 102\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    105\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[0;32m    900\u001b[0m             array,\n\u001b[0;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    904\u001b[0m         )\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class Linear_Regression():\n",
    "\n",
    "    '''\n",
    "    linear regression using gradient descent\n",
    "    '''\n",
    "    def __init__(self, max_iter = 1e5, alpha = 0.01, eps = 1e-10, verbose= 0):\n",
    "        '''\n",
    "        :param verbose: set 1 to display more details of J val changes\n",
    "        '''\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose       \n",
    "        \n",
    "    def h(self, b, w, X): \n",
    "        '''\n",
    "        :param b -  float or ndarry of shape [m,1], m - number of samples\n",
    "        :param w - ndarray of shape [1,m],  n - number of features\n",
    "        :param X - ndarray of shape [m,n], m - number of samples, n - number of features\n",
    "        '''\n",
    "        assert (X.shape[1]== w.shape[1])\n",
    "\n",
    "        # YOUR_CODE. Insert the expression of h developed in Linear_Regression_1\n",
    "        # START_CODE \n",
    "        h_res =  b + X @ w.T \n",
    "        # END_CODE \n",
    "        \n",
    "        if h_res.shape != (X.shape[0],1):\n",
    "            print('h.shape = {} but expected {}'.format (h_res.shape,  (self.m,1)))\n",
    "            raise Exception('Check assertion in h')    \n",
    "        return h_res\n",
    "\n",
    "    def J (self, h, y):      \n",
    "        '''\n",
    "        :param h - ndarray of shape (m,1)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return expression for cost function \n",
    "        '''\n",
    "        if h.shape !=y.shape:\n",
    "            print('h.shape = {} does not match y.shape = {}.Expected {}'.format (h.shape, y.shape, (self.m,1)))\n",
    "            raise Exception('Check assertion in J')   \n",
    "        # YOUR_CODE. Insert the expression of J developed in Linear_Regression_2\n",
    "        # START_CODE \n",
    "        J_res= 1/(2*m)*np.sum((h-y)**2)\n",
    "        # END_CODE \n",
    "\n",
    "        return J_res\n",
    "        \n",
    "    def J_derivative(self, params, X, y): \n",
    "        '''\n",
    "        :param params - tuple (b,w), where w is the 2d ndarry of shape (1,n), n- number of features \n",
    "        :param X- ndarray of shape (m, n)\n",
    "        :param y - ndarray of shape (m,1)\n",
    "        :return tuple of derivatrives of cost function by b and w\n",
    "        '''\n",
    "      \n",
    "        b,w = params\n",
    "        assert (w.shape == (1,self.n))                \n",
    "        h_val = self.h(b,w,X)\n",
    "        if  h_val.shape != (self.m, 1):\n",
    "            print('h.shape = {}, but expected {}'.format (h_val.shape, (self.m, 1)))\n",
    "            raise Exception('Check assertion in J_derivative')\n",
    "        \n",
    "        # YOUR_CODE. Insert the expressions for derivates of J by b and by w to dJ_b and dJ_w developed in Linear_Regression_3\n",
    "        # START_CODE             \n",
    "        dJ_b= 1/m * np.sum(h_val-y)\n",
    "        dJ_w= 1/m * (h_val-y).T @ X\n",
    "        # END_CODE\n",
    "        \n",
    "        return (dJ_b, dJ_w)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X - ndarray training set of shape [m,n], m - number of samples, n - number of features\n",
    "        :param y - ndarray - 1d array \n",
    "        :return: True in case of successful fit \n",
    "        '''      \n",
    "        if self.verbose: \n",
    "            print ('Running gradient descent with alpha = {}, eps= {}, max_iter= {}'.format(\n",
    "                self.alpha, self.eps, self.max_iter))\n",
    "        self.m,self.n= X.shape # number of samples, number of features  \n",
    "        y = y.reshape(self.m,1) # make it 2 d to make sure it corresponds to h_val\n",
    "        b = 0 # init intercept with 0\n",
    "        w= np.zeros(self.n).reshape(1,-1) # make sure it's shape is [1,n]\n",
    "        params = (b,w)\n",
    "        \n",
    "        self.J_hist=[-1] # used for keeping J values. Init with -1 to avoid 0 at first iter\n",
    "        continue_iter = True # flag to continue next iter (grad desc step)\n",
    "        iter_number =0 # used for limit by max_iter\n",
    "\n",
    "        while continue_iter:            \n",
    "            # Do step of gradient descent    \n",
    "            # YOUR_CODE. Insert one step of gradien descent developed in Linear_Regression_4 \n",
    "            # START_CODE \n",
    "            dJ_b, dJ_w = 1/m * np.sum(self.h(b,w,X)-y), 1/m * (self.h(b,w,X)-y).T @ X\n",
    "            b = b - self.alpha * dJ_b\n",
    "            w = w - self.alpha * dJ_w\n",
    "            params= b,w\n",
    "            # END_CODE \n",
    "            \n",
    "            # keep history of J values\n",
    "            self.J_hist.append(self.J(self.h(b, w, X), y))\n",
    "            if self.verbose:\n",
    "                print ('b = {}, w= {}, J= {}'.format(b,w,self.J_hist[-1]))\n",
    "            # check criteria of exit the loop (finish grad desc)\n",
    "            if self.max_iter and iter_number> self.max_iter: # if max_iter is provided and limit succeeded\n",
    "                continue_iter = False\n",
    "            elif np.abs(self.J_hist[iter_number-1] - self.J_hist[iter_number])< self.eps: # if accuracy is succeeded\n",
    "                continue_iter = False\n",
    "            iter_number += 1\n",
    "            \n",
    "        # store the final params to further using \n",
    "        self.intercept_, self.coef_= params        \n",
    "        return True        \n",
    "        \n",
    "    def draw_cost_changes(self):        \n",
    "        J_hist= self.J_hist[1:]\n",
    "        plt.figure()\n",
    "        plt.scatter(np.arange(0,len(J_hist)),J_hist,s=20,marker='.',c='b')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost function J value')\n",
    "        title_str = 'Complited: {}, alpha ={}, max_iter={}, eps={}'.format( len(self.J_hist)-2, self.alpha, self.max_iter,self.eps)\n",
    "        # Note: len(J_hist)-2) due to first one is -1 (was not iteration), iter + 1  at the end  of the gradient loop\n",
    "        plt.title(title_str)\n",
    " \n",
    "\n",
    "    def predict(self, X): \n",
    "        '''\n",
    "        :param X - ndarray of shape (?,n)\n",
    "        :return \n",
    "        '''\n",
    "        return self.h(self.intercept_, self.coef_, X)\n",
    "        \n",
    "   \n",
    "    def score(self, X_test, y_test):\n",
    "        '''\n",
    "        :param X_test - ndarray testing set or any for prediction of shape [?,n], ? - number of samples, n - number of features\n",
    "        :param y_test - ndarray - 1d array \n",
    "        :return R2 score of y_test and prediction for X_test\n",
    "        '''\n",
    "        z= self.predict(X_test)\n",
    "        from sklearn.metrics import r2_score\n",
    "        return (r2_score(y_test, z))\n",
    "\n",
    "        \n",
    "# DON'T_CHANGE_THIS_CODE. It is used to let you check the result is correct \n",
    "\n",
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('y_train.shape= ',y_train.shape)\n",
    "print ('X_train= \\n{}'.format (X_train[:5,:]))\n",
    "lin_reg = Linear_Regression(alpha= 0.01, verbose=0, eps=1e-8)\n",
    "lin_reg.fit (X_train, y_train)\n",
    "lin_reg.draw_cost_changes()\n",
    "print ('R2 Score =', lin_reg.score(X_test, y_test))\n",
    "print ('b: {}, w= {}'.format(lin_reg.intercept_, lin_reg.coef_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit-metrics-scorer (from versions: none)\n",
      "ERROR: No matching distribution found for scikit-metrics-scorer\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f17e58a4649de71ddaa6ebe913363adc4ba35f246eec8b95e969dd0a5f14125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
